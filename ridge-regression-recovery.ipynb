{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression Recovery\n",
    "\n",
    "This paper https://arxiv.org/abs/1610.03045 gives an iteratvie method for solving the Ridge regression problem up to $\\epsilon$ precision using random projections.  The key idea is that a relationship between the primal and dual can be exploited to reduce dimensionality on both the sample size and number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm, eig\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rng = np.random.RandomState(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random projections used in the paper come from a family called _Hessian Sketches_.  Details are omitted here but for now it is enough just to apply a Gaussian matrix as the Hessian sketch of suitable dimension.  From https://arxiv.org/abs/1501.06195 it is suggested that if a Gaussian matrix has sufficiently large dimension, namely at least $c d_n$ where $d_n$ is some index between 1 and $n$ denoting the index of the largest eigenvalue which is upper bounded by the ctrical radius of the data.  It is also referred to in https://arxiv.org/abs/1411.0347 as the _Gaussian Width_ which for $n \\times d$ data can be bounded as $O(d)$.  I assume this is the same as for the JLT condition that we need something like $O(d/\\epsilon^2 \\text{poly log}(d))$. So for now we focus on two sketches $\\Pi$ and $R$ which will reduce the input size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Hessian_Sketch(num_rows, num_cols):\n",
    "    '''Returns a (num_rows)*(num_cols) matrix which is the \n",
    "    Hessian sketch.  Future functionality to extend from \n",
    "    Gaussian to ROS sketches which are faster to apply.'''\n",
    "    return np.random.randn(num_rows, num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_primal_dual_sketch(data, targets,\n",
    "                                 offset, tolerance,\n",
    "                                max_iterations=10):\n",
    "    '''Perform the vanilla iterative_primal_dual_sketching.'''\n",
    "    X = data\n",
    "    y = targets\n",
    "    _lambda = offset \n",
    "    n, d = X.shape\n",
    "    is_converged = False\n",
    "    \n",
    "    \n",
    "    \n",
    "    # sketch parameters.  Do need to satisfy certain bounds\n",
    "    p = d//2\n",
    "    m = n//2\n",
    "    # initialise sketches nb. I have switched notation from the R in paper\n",
    "    feature_sketch = Hessian_Sketch(d, p)\n",
    "    \n",
    "    sample_sketch = Hessian_Sketch(m,n) \n",
    "    # same as in paper for Pi but needs to be transposed for  right multiplication\n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "   \n",
    "    X_R = np.dot(X,feature_sketch) # just renaming to avoid unnecessary compute\n",
    "    double_sketch = (0.5*n)*np.dot(sample_sketch, X_R)\n",
    "    \n",
    "    ### parameters for iteration\n",
    "    w_hat = np.zeros(shape=(d,1))\n",
    "    #u = np.random.randn(p,1)\n",
    "    \n",
    "    #print(\"X_R shape {}\".format(X_R.shape))\n",
    "    #print(\"X_Rz shape {}\".format(np.dot(X_R,z).shape))\n",
    "    #print(\"y shape {}\".format(y.shape))\n",
    "    #print(\"X w_hat shape {}\".format(np.dot(X, w_hat).shape))\n",
    "    \n",
    "    \n",
    "    for t in range(max_iterations):\n",
    "        k = 0\n",
    "        z = np.ravel(np.ones(shape=(p,1)))\n",
    "        #z = np.ravel(np.random.randn(p,1))\n",
    "        w_hat = np.ravel(w_hat)\n",
    "        while not is_converged:\n",
    "            print(\"iteration on (t,k) ({},{})\".format(t,k))\n",
    "            print(\"norm(z) = {}\".format(norm(z,2)))\n",
    "            #print(\"Start loop shape {}\".format(z.shape))\n",
    "            # this is just problem set up\n",
    "            \n",
    "            print(\"X_R shape {}\".format(X_R.shape))\n",
    "            print(\"y shape {}\".format(y.shape))\n",
    "            print(\"X shape {}\".format(X.shape))\n",
    "            print(\"w_hat shape {}\".format(w_hat.shape))\n",
    "            temp1 = (1/n)*(np.dot(X_R.T, y) - np.dot(X_R.T, np.dot(X,w_hat)) - np.dot(X_R.T, np.dot(X_R,z)))\n",
    "            print(\"feature ksetch shape {}\".format(feature_sketch.shape))\n",
    "            print(\"z shape {}\".format(z.shape))\n",
    "            temp2 = _lambda*np.dot(feature_sketch.T, w_hat) + _lambda*z\n",
    "            temp3 = np.ravel(temp1 - temp2) # remove column dimension\n",
    "            print(\"temp1 shape {}\".format(temp1.shape))\n",
    "            print(\"temp2 shape {}\".format(temp2.shape))\n",
    "            print(\"temp3 shape {}\".format(temp3[:,None].shape))\n",
    "            \n",
    "            \n",
    "            #####################\n",
    "            #def linear_function(u):\n",
    "            #    sketched_vector = np.dot(double_sketch,u)\n",
    "            #    #print(\"Sketched vector shape {}\".format(sketched_vector.shape))\n",
    "            #    regulariser = 0.5*_lambda*norm(u,2)**2\n",
    "            #    inner_product = np.dot(temp3,u)\n",
    "            #    term1 = norm(sketched_vector,2)**2\n",
    "            #    return term1 + regulariser - inner_product\n",
    "\n",
    "            #def grad(u):\n",
    "             #   return 2*np.dot(double_sketch.T, np.dot(double_sketch, u))\\\n",
    "              #          +_lambda*u + temp3\n",
    "            ###################\n",
    "            \n",
    "            print(\"Covariance shape {}\".format(np.dot(double_sketch.T,double_sketch).shape))\n",
    "            print(\"temp3 shape {}\".format(temp3[:,None].shape))\n",
    "            z_hat = np.linalg.lstsq(2*np.dot(double_sketch.T,double_sketch) \\\n",
    "                                    +_lambda*np.eye(p),temp3[:,None])[0]\n",
    "            print(\"z_hat shape {}\".format(z_hat.shape))\n",
    "            \n",
    "            \n",
    "            \n",
    "            #result = scipy.optimize.minimize(fun=linear_function, x0=z,\n",
    "                                          #method=\"Newton-CG\", jac=grad, tol=1e-5)\n",
    "            #print(\"norm of Gradient at norm(z) = {} is {}\".format(norm(z), norm(grad(z),2)))\n",
    "            z_new = z[:,None] + z_hat \n",
    "            print(\"z_new shape {}\".format(z_new.shape))\n",
    "            print(\"norm(z_new) = {}\".format(norm(z_new)))\n",
    "            #print(\"Type result.x {}\".format(type(result.x)))\n",
    "            #print(\"Result.x shape {}\".format(result.x.shape))\n",
    "            #print(\"End loop shape {}\".format(z_new.shape))\n",
    "            #print(\"End loop shape {}\".format(z.shape))\n",
    "            #print(norm(z_new - z,2))\n",
    "            if norm(z_new - z,2) < tolerance:\n",
    "                z = z_new\n",
    "                print(\"Residual: {}\".format(norm(z_new - z,2)))\n",
    "                print(\"Exiting loop.\")\n",
    "                is_converged = True\n",
    "            else:\n",
    "                z = z_new # flip the labelling so can just plug z in above code\n",
    "                k += 1\n",
    "                print(\"Looping again.\")\n",
    "            #print(\"norm of Gradient at norm(z) = {}\".format(norm(z)))\n",
    "            \n",
    "        \n",
    "        \n",
    "        is_converged = False # reset so the loop is entered next time over    \n",
    "        # Update dual  \n",
    "        print(np.dot(X, w_hat).shape)\n",
    "        print(z.shape)\n",
    "        print(X_R.shape)\n",
    "        print(w_hat.shape)\n",
    "        print(\"y shape {}\".format(y.shape))\n",
    "        alpha_dual = y[:, None] - np.dot(X, w_hat[:,None]) - np.dot(X_R, z)\n",
    "        print('Alpha shape {}'.format(alpha_dual.shape))\n",
    "        \n",
    "        # Update primal\n",
    "        w_hat = (1/(n*_lambda))*np.dot(X.T, alpha_dual)\n",
    "        print(w_hat.shape)\n",
    "    return w_hat, alpha_dual\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration on (t,k) (0,0)\n",
      "norm(z) = 22.360679774997898\n",
      "X_R shape (5000, 500)\n",
      "y shape (5000,)\n",
      "X shape (5000, 1000)\n",
      "w_hat shape (1000,)\n",
      "feature ksetch shape (1000, 500)\n",
      "z shape (500,)\n",
      "temp1 shape (500,)\n",
      "temp2 shape (500,)\n",
      "temp3 shape (500, 1)\n",
      "Covariance shape (500, 500)\n",
      "temp3 shape (500, 1)\n",
      "z_hat shape (500, 1)\n",
      "z_new shape (500, 1)\n",
      "norm(z_new) = 22.36067977499778\n",
      "Residual: 0.0\n",
      "Exiting loop.\n",
      "(5000,)\n",
      "(500, 1)\n",
      "(5000, 500)\n",
      "(1000,)\n",
      "y shape (5000,)\n",
      "Alpha shape (5000, 1)\n",
      "(1000, 1)\n",
      "iteration on (t,k) (1,0)\n",
      "norm(z) = 22.360679774997898\n",
      "X_R shape (5000, 500)\n",
      "y shape (5000,)\n",
      "X shape (5000, 1000)\n",
      "w_hat shape (1000,)\n",
      "feature ksetch shape (1000, 500)\n",
      "z shape (500,)\n",
      "temp1 shape (500,)\n",
      "temp2 shape (500,)\n",
      "temp3 shape (500, 1)\n",
      "Covariance shape (500, 500)\n",
      "temp3 shape (500, 1)\n",
      "z_hat shape (500, 1)\n",
      "z_new shape (500, 1)\n",
      "norm(z_new) = 22.360679788172764\n",
      "Looping again.\n",
      "iteration on (t,k) (1,1)\n",
      "norm(z) = 22.360679788172764\n",
      "X_R shape (5000, 500)\n",
      "y shape (5000,)\n",
      "X shape (5000, 1000)\n",
      "w_hat shape (1000,)\n",
      "feature ksetch shape (1000, 500)\n",
      "z shape (500, 1)\n",
      "temp1 shape (500, 500)\n",
      "temp2 shape (500, 500)\n",
      "temp3 shape (250000, 1)\n",
      "Covariance shape (500, 500)\n",
      "temp3 shape (250000, 1)\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "Incompatible dimensions",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-d874e570cb0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miterative_primal_dual_sketch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-96-9e51899d599d>\u001b[0m in \u001b[0;36miterative_primal_dual_sketch\u001b[0;34m(data, targets, offset, tolerance, max_iterations)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Covariance shape {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdouble_sketch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdouble_sketch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"temp3 shape {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mz_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstsq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdouble_sketch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdouble_sketch\u001b[0m\u001b[0;34m)\u001b[0m                                     \u001b[0;34m+\u001b[0m\u001b[0m_lambda\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtemp3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"z_hat shape {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36mlstsq\u001b[0;34m(a, b, rcond)\u001b[0m\n\u001b[1;32m   1908\u001b[0m     \u001b[0mldb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1909\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1910\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Incompatible dimensions'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1911\u001b[0m     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_commonType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1912\u001b[0m     \u001b[0mresult_real_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_realType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLinAlgError\u001b[0m: Incompatible dimensions"
     ]
    }
   ],
   "source": [
    "w, alpha = iterative_primal_dual_sketch(X, y, offset=1e-5, tolerance=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y, coef = make_regression(n_samples=5000, n_features=1000, n_informative=1000, n_targets=1, coef=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
