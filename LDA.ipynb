{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some code to understand how LDA works and see if we can apply basic sketching techniques.  The sample documents are given below (from https://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_a = \"Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\"\n",
    "doc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
    "doc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
    "doc_d = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\n",
    "doc_e = \"Health professionals say that brocolli is good for your health.\"\n",
    "\n",
    "# compile sample documents into a list\n",
    "doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cleaning steps are to (1) _tokenize_: converting a document to atomic elements, (2) _stopping_: removing meaningless words, (3) _Stemming_: Merging words of equivalent meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Tokenization\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nltk.tokenize` module will match word characters until it reaches non-word characters e.g a space.  This is simple but can cause issues for words with apostrophes etc.  This needs to be improved for a more accurate tokenization.  This is for a single document but for all documents a loop will be necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brocolli', 'is', 'good', 'to', 'eat', 'my', 'brother', 'likes', 'to', 'eat', 'good', 'brocolli', 'but', 'not', 'my', 'mother']\n"
     ]
    }
   ],
   "source": [
    "raw = doc_a.lower()\n",
    "tokens = tokenizer.tokenize(raw)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2. Stopping - removing meaningless words\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "# english stop words\n",
    "en_stop = get_stop_words('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brocolli', 'good', 'eat', 'brother', 'likes', 'eat', 'good', 'brocolli', 'mother']\n"
     ]
    }
   ],
   "source": [
    "# removing stop words:\n",
    "stopped_tokens = [word for word in tokens\n",
    "                      if not word in en_stop]\n",
    "print(stopped_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3. Stemming - combining similar words\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# create p_stemmer of class porterStemmer\n",
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brocolli', 'good', 'eat', 'brother', 'like', 'eat', 'good', 'brocolli', 'mother']\n"
     ]
    }
   ],
   "source": [
    "# p_stemmer requires tokens input as str type\n",
    "texts = [p_stemmer.stem(word) for word in stopped_tokens]\n",
    "\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do full process in one function\n",
    "\n",
    "def clean_document(doc):\n",
    "    \"\"\"\n",
    "    Tokenizes, stops, and stems the doc.\n",
    "    Returns the tokens.\"\"\"\n",
    "    raw = doc.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    \n",
    "    en_stop = get_stop_words('en')\n",
    "    stopped_tokens = [word for word in tokens\n",
    "                      if not word in en_stop]\n",
    "    \n",
    "    p_stemmer = PorterStemmer()\n",
    "    texts = [p_stemmer.stem(word) for word in stopped_tokens]\n",
    "\n",
    "    return texts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['brocolli',\n",
       "  'good',\n",
       "  'eat',\n",
       "  'brother',\n",
       "  'like',\n",
       "  'eat',\n",
       "  'good',\n",
       "  'brocolli',\n",
       "  'mother'],\n",
       " ['mother',\n",
       "  'spend',\n",
       "  'lot',\n",
       "  'time',\n",
       "  'drive',\n",
       "  'brother',\n",
       "  'around',\n",
       "  'basebal',\n",
       "  'practic'],\n",
       " ['health',\n",
       "  'expert',\n",
       "  'suggest',\n",
       "  'drive',\n",
       "  'may',\n",
       "  'caus',\n",
       "  'increas',\n",
       "  'tension',\n",
       "  'blood',\n",
       "  'pressur'],\n",
       " ['often',\n",
       "  'feel',\n",
       "  'pressur',\n",
       "  'perform',\n",
       "  'well',\n",
       "  'school',\n",
       "  'mother',\n",
       "  'never',\n",
       "  'seem',\n",
       "  'drive',\n",
       "  'brother',\n",
       "  'better'],\n",
       " ['health', 'profession', 'say', 'brocolli', 'good', 'health']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_texts = [[] for i in range(len(doc_set))]\n",
    "\n",
    "for doc_number in range(len(doc_set)):\n",
    "    clean_texts[doc_number] = clean_document(\n",
    "                        doc_set[doc_number])\n",
    "    \n",
    "clean_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Term Matrix\n",
    "The result of the cleaning stage is `texts`, a tokenized, stopped, and stemmed list of words from a single document.  If we had done this for every document then `texts` would be a list of lists.\n",
    "\n",
    "To generate the LDA model we need to count how frequently each term occurs in eah document by creating a document-term matrix using `gensim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models\n",
    "\n",
    "dictionary = corpora.Dictionary(clean_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(32 unique tokens: ['brocolli', 'brother', 'eat', 'good', 'like']...)\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Dictionary` function looks over all of `clean_texts` and assigns a uniqu integer id to eqch unique token (word) whilst also collecting word counts and statistics.  We can view a token's unique id as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'brocolli': 0, 'brother': 1, 'eat': 2, 'good': 3, 'like': 4, 'mother': 5, 'around': 6, 'basebal': 7, 'drive': 8, 'lot': 9, 'practic': 10, 'spend': 11, 'time': 12, 'blood': 13, 'caus': 14, 'expert': 15, 'health': 16, 'increas': 17, 'may': 18, 'pressur': 19, 'suggest': 20, 'tension': 21, 'better': 22, 'feel': 23, 'never': 24, 'often': 25, 'perform': 26, 'school': 27, 'seem': 28, 'well': 29, 'profession': 30, 'say': 31}\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For later work, this might be the point at which we can apply hashing for CountMin sketch.  However for now, carry on and convert to bag of words model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in clean_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 2), (1, 1), (2, 2), (3, 2), (4, 1), (5, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 2), (1, 1), (2, 2), (3, 2), (4, 1), (5, 1)], [(1, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1)], [(8, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1)], [(1, 1), (5, 1), (8, 1), (19, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1)], [(0, 1), (3, 1), (16, 2), (30, 1), (31, 1)]]\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`corpus` is now a list of vectors equal to the number of documents.  For every document the vector is a series of tuples.  The list of tuples representing the first document is `corpus[0]`.  Each tuple is `(term_id, term_frequency)` pair - if `dictionary.token2id` is 0 then the first tuple indicates that `brocolli` appeared twice in `doc_a`.\n",
    "\n",
    "`doc2bow()` only includes terms that occur and does not show those that do not appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 2), (1, 1), (2, 2), (3, 2), (4, 1), (5, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying LDA\n",
    "Now we have generate `corpus` which is a document term matrix we can start the LDA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=3,\n",
    "                                          id2word=dictionary, passes=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "1. `num_topics` - _required_. An LDA model needs the user to determine how many topics should be generated (as part of the generative model procedure).  This document set is small so only ask for three topics.\n",
    "2. `id2word` - _required_. The `LdaModel` class requires the previous dictionary to map ids to strings.\n",
    "3. `passes` - _optional_. Determines how many passes over the `corpus` that will be taken.  For later streaming application we will want this to be 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.068*\"drive\" + 0.068*\"lot\" + 0.068*\"spend\"'), (1, '0.125*\"health\" + 0.050*\"pressur\" + 0.050*\"suggest\"'), (2, '0.074*\"good\" + 0.074*\"brocolli\" + 0.074*\"brother\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics(num_topics=3, num_words=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each generated topic is separated by a comma and each topic shows the three most probable words to occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary, passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.082*\"health\" + 0.048*\"mother\" + 0.048*\"brother\" + 0.048*\"drive\"'), (1, '0.054*\"good\" + 0.054*\"brocolli\" + 0.053*\"drive\" + 0.053*\"brother\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics(num_topics=2, num_words=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA comes from a generative model which assumes a document is built as follows:\n",
    "1. Choose the number of words in a document.\n",
    "2. Choose the number of topics from which words can be chosen in the document.\n",
    "3. Using the distribution of each topic, choose a word from the distribution to fill a slot.\n",
    "\n",
    "LDA then backtracks and finds which topics would have created the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
